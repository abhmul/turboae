Namespace(batch_size=10000, bce_lambda=1.0, bec_p=0.0, bec_p_dec=0.0, ber_lambda=1.0, block_len=100, block_len_high=200, block_len_low=10, bsc_p=0.0, bsc_p_dec=0.0, channel='t-dist', code_rate_k=1, code_rate_n=3, dec_act='linear', dec_kernel_size=5, dec_lr=0.0001, dec_num_layer=5, dec_num_unit=100, dec_rnn='gru', decoder='TurboAE_rate3_cnn', demod_lr=0.005, demod_num_layer=1, demod_num_unit=20, dropout=0.0, enc_act='elu', enc_clipping='both', enc_grad_limit=0.01, enc_kernel_size=5, enc_lr=0.0001, enc_num_layer=2, enc_num_unit=100, enc_quantize_level=2, enc_rnn='gru', enc_truncate_limit=0, enc_value_limit=1.0, encoder='TurboAE_rate3_cnn', extrinsic=1, focal_alpha=1.0, focal_gamma=0.0, img_size=10, init_nw_weight='./models/dta_steq2_cnn2_cnn5_enctrain2_dectrainneg15_2.pt', is_interleave=1, is_k_same_code=False, is_parallel=1, is_same_interleaver=1, is_variable_block_len=False, joint_train=0, k_same_code=2, lambda_maxBCE=0.01, loss='bce', mod_lr=0.005, mod_num_layer=1, mod_num_unit=20, mod_pc='block_power', mod_rate=2, momentum=0.9, no_code_norm=False, no_cuda=False, num_ber_puncture=5, num_block=20000, num_epoch=0, num_iter_ft=5, num_iteration=6, num_train_dec=5, num_train_demod=5, num_train_enc=1, num_train_mod=1, optimizer='adam', precompute_norm_stats=False, print_pos_ber=False, print_pos_power=False, print_test_traj=True, radar_power=5.0, radar_prob=0.05, rec_quantize=False, rec_quantize_level=2, rec_quantize_limit=1.0, snr_points=13, snr_test_end=3.0, snr_test_start=-3.0, test_channel_mode='block_norm_ste', test_ratio=1, train_channel_mode='block_norm_ste', train_dec_channel_high=2.0, train_dec_channel_low=-1.5, train_enc_channel_high=1.0, train_enc_channel_low=1.0, vv=3.0)
using random interleaver [26 86  2 55 75 93 16 73 54 95 53 92 78 13  7 30 22 24 33  8 43 62  3 71
 45 48  6 99 82 76 60 80 90 68 51 27 18 56 63 74  1 61 42 41  4 15 17 40
 38  5 91 59  0 34 28 50 11 35 23 52 10 31 66 57 79 85 32 84 14 89 19 29
 49 97 98 69 20 94 72 77 25 37 81 46 39 65 58 12 88 70 87 36 21 83  9 96
 67 64 47 44] [18 29 64 92 72 87  5 15 12 17 61 76  9 78 80  7 33  6 37 74 79  1 45 28
 60 52 25 39 97 44 16 55 83 49 22 70 47  4 82 94 53 66 26 84 31 63  8 75
 98 57 71 99 86 96 69 24 30 13 40 56 68 95 81 19 38 91 54 32 51 85 11 89
 90 36 65 88 41 14 27 50 20 46 67 35 62  2 59 23 58 43 10  0 73 21 77 42
  3 93 48 34]
Channel_AE(
  (enc): ENC_interCNN(
    (enc_cnn_1): DataParallel(
      (module): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(1, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (enc_cnn_2): DataParallel(
      (module): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(1, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (enc_cnn_3): DataParallel(
      (module): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(1, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (enc_linear_1): DataParallel(
      (module): Linear(in_features=100, out_features=1, bias=True)
    )
    (enc_linear_2): DataParallel(
      (module): Linear(in_features=100, out_features=1, bias=True)
    )
    (enc_linear_3): DataParallel(
      (module): Linear(in_features=100, out_features=1, bias=True)
    )
    (interleaver): Interleaver()
  )
  (dec): DEC_LargeCNN(
    (interleaver): Interleaver()
    (deinterleaver): DeInterleaver()
    (dec1_cnns): ModuleList(
      (0): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (1): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (2): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (3): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (4): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (5): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
    )
    (dec2_cnns): ModuleList(
      (0): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (1): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (2): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (3): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (4): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
      (5): DataParallel(
        (module): SameShapeConv1d(
          (cnns): ModuleList(
            (0): Conv1d(7, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
            (4): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))
          )
        )
      )
    )
    (dec1_outputs): ModuleList(
      (0): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (1): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (2): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (3): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (4): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (5): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
    )
    (dec2_outputs): ModuleList(
      (0): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (1): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (2): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (3): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (4): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (5): DataParallel(
        (module): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
test loss trajectory []
test ber trajectory []
total epoch 0
SNRS [-3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
-3.0
generating noise for atn
atn_sigma: 1.4125375446227544
atn_snr: -3.0000000000000004
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.4125375446227544
-3.0
generating noise for atn
atn_sigma: 1.4125375446227544
atn_snr: -3.0000000000000004
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.4125375446227544
no pos BER specified.
Test SNR -3.0 with ber  0.06551699340343475 with bler 0.7144999999999999
Test SNR -3.0 with ber var 0.0075627705082297325 with bler var 0.20399994999749993
No puncturation is there.
-2.5
generating noise for atn
atn_sigma: 1.333521432163324
atn_snr: -2.4999999999999996
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.333521432163324
-2.5
generating noise for atn
atn_sigma: 1.333521432163324
atn_snr: -2.4999999999999996
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.333521432163324
no pos BER specified.
Test SNR -2.5 with ber  0.042837999761104584 with bler 0.61965
Test SNR -2.5 with ber var 0.00465675862506032 with bler var 0.23569566228311412
No puncturation is there.
-2.0
generating noise for atn
atn_sigma: 1.2589254117941673
atn_snr: -2.0
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.2589254117941673
-2.0
generating noise for atn
atn_sigma: 1.2589254117941673
atn_snr: -2.0
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.2589254117941673
no pos BER specified.
Test SNR -2.0 with ber  0.027749501168727875 with bler 0.5152
Test SNR -2.0 with ber var 0.002696565119549632 with bler var 0.24978144907245364
No puncturation is there.
-1.5
generating noise for atn
atn_sigma: 1.1885022274370185
atn_snr: -1.5000000000000004
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.1885022274370185
-1.5
generating noise for atn
atn_sigma: 1.1885022274370185
atn_snr: -1.5000000000000004
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.1885022274370185
no pos BER specified.
Test SNR -1.5 with ber  0.017489001154899597 with bler 0.42005000000000003
Test SNR -1.5 with ber var 0.0013891443377360702 with bler var 0.24362017850892542
No puncturation is there.
-1.0
generating noise for atn
atn_sigma: 1.1220184543019633
atn_snr: -0.9999999999999991
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.1220184543019633
-1.0
generating noise for atn
atn_sigma: 1.1220184543019633
atn_snr: -0.9999999999999991
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.1220184543019633
no pos BER specified.
Test SNR -1.0 with ber  0.01114100031554699 with bler 0.33089999999999997
Test SNR -1.0 with ber var 0.0007159638917073607 with bler var 0.22141626081304075
No puncturation is there.
-0.5
generating noise for atn
atn_sigma: 1.0592537251772889
atn_snr: -0.49999999999999994
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.0592537251772889
-0.5
generating noise for atn
atn_sigma: 1.0592537251772889
atn_snr: -0.49999999999999994
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.0592537251772889
no pos BER specified.
Test SNR -0.5 with ber  0.007261999882757664 with bler 0.2636
Test SNR -0.5 with ber var 0.0003719419473782182 with bler var 0.19412474623731185
No puncturation is there.
0.0
generating noise for atn
atn_sigma: 1.0
atn_snr: -0.0
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.0
0.0
generating noise for atn
atn_sigma: 1.0
atn_snr: -0.0
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 1.0
no pos BER specified.
Test SNR 0.0 with ber  0.00491249980404973 with bler 0.2066
Test SNR 0.0 with ber var 0.0001970221783267334 with bler var 0.16392463623181155
No puncturation is there.
0.5
generating noise for atn
atn_sigma: 0.9440608762859234
atn_snr: 0.4999999999999999
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.9440608762859234
0.5
generating noise for atn
atn_sigma: 0.9440608762859234
atn_snr: 0.4999999999999999
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.9440608762859234
no pos BER specified.
Test SNR 0.5 with ber  0.003461000043898821 with bler 0.16305
Test SNR 0.5 with ber var 0.00011483722482807934 with bler var 0.13647152107605381
No puncturation is there.
1.0
generating noise for atn
atn_sigma: 0.8912509381337456
atn_snr: 0.9999999999999997
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.8912509381337456
1.0
generating noise for atn
atn_sigma: 0.8912509381337456
atn_snr: 0.9999999999999997
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.8912509381337456
no pos BER specified.
Test SNR 1.0 with ber  0.002466500038281083 with bler 0.12539999999999998
Test SNR 1.0 with ber var 7.432509300997481e-05 with bler var 0.10968032401620083
No puncturation is there.
1.5
generating noise for atn
atn_sigma: 0.8413951416451951
atn_snr: 1.4999999999999993
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.8413951416451951
1.5
generating noise for atn
atn_sigma: 0.8413951416451951
atn_snr: 1.4999999999999993
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.8413951416451951
no pos BER specified.
Test SNR 1.5 with ber  0.0018505000043660402 with bler 0.0996
Test SNR 1.5 with ber var 5.725351002183743e-05 with bler var 0.0896843242162108
No puncturation is there.
2.0
generating noise for atn
atn_sigma: 0.7943282347242815
atn_snr: 1.9999999999999998
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.7943282347242815
2.0
generating noise for atn
atn_sigma: 0.7943282347242815
atn_snr: 1.9999999999999998
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.7943282347242815
no pos BER specified.
Test SNR 2.0 with ber  0.0013649999164044857 with bler 0.0776
Test SNR 2.0 with ber var 3.4758511901600286e-05 with bler var 0.07158181909095455
No puncturation is there.
2.5
generating noise for atn
atn_sigma: 0.7498942093324559
atn_snr: 2.499999999999999
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.7498942093324559
2.5
generating noise for atn
atn_sigma: 0.7498942093324559
atn_snr: 2.499999999999999
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.7498942093324559
no pos BER specified.
Test SNR 2.5 with ber  0.0009790000040084124 with bler 0.059300000000000005
Test SNR 2.5 with ber var 2.1852649297215976e-05 with bler var 0.05578629931496574
No puncturation is there.
3.0
generating noise for atn
atn_sigma: 0.7079457843841379
atn_snr: 3.0
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.7079457843841379
3.0
generating noise for atn
atn_sigma: 0.7079457843841379
atn_snr: 3.0
shape torch.Size([10000, 100, 1])
self.v: 3.0
self.sigma: 0.7079457843841379
no pos BER specified.
Test SNR 3.0 with ber  0.0007319999858736992 with bler 0.04675
Test SNR 3.0 with ber var 1.5404946680064313e-05 with bler var 0.04456666583329167
No puncturation is there.
final results on SNRs  [-3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
BER [0.06551699340343475, 0.042837999761104584, 0.027749501168727875, 0.017489001154899597, 0.01114100031554699, 0.007261999882757664, 0.00491249980404973, 0.003461000043898821, 0.002466500038281083, 0.0018505000043660402, 0.0013649999164044857, 0.0009790000040084124, 0.0007319999858736992]
BER_VAR [0.0075627705082297325, 0.00465675862506032, 0.002696565119549632, 0.0013891443377360702, 0.0007159638917073607, 0.0003719419473782182, 0.0001970221783267334, 0.00011483722482807934, 7.432509300997481e-05, 5.725351002183743e-05, 3.4758511901600286e-05, 2.1852649297215976e-05, 1.5404946680064313e-05]
BLER [0.7144999999999999, 0.61965, 0.5152, 0.42005000000000003, 0.33089999999999997, 0.2636, 0.2066, 0.16305, 0.12539999999999998, 0.0996, 0.0776, 0.059300000000000005, 0.04675]
BLER_VAR [0.20399994999749993, 0.23569566228311412, 0.24978144907245364, 0.24362017850892542, 0.22141626081304075, 0.19412474623731185, 0.16392463623181155, 0.13647152107605381, 0.10968032401620083, 0.0896843242162108, 0.07158181909095455, 0.05578629931496574, 0.04456666583329167]
encoder power is tensor(0.9998)
adjusted SNR should be [-3.0018443043167506, -2.5018442655691113, -2.0018439807261044, -1.5018439070913006, -1.001843646833302, -0.501844112261875, -0.0018439206471627249, 0.49815607027307474, 0.9981561578227098, 1.4981561299369817, 1.9981565785288509, 2.4981562521162353, 2.9981561113667183]
